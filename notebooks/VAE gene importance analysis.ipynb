{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input, Lambda, BatchNormalization, Activation, Dropout, concatenate\n",
    "from tensorflow.keras.losses import mse\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from time import time\n",
    "from tensorflow.python.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.losses import mse, binary_crossentropy\n",
    "\n",
    "from IPython.display import SVG\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.python.keras.callbacks import TensorBoard\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "parallelization_factor = 10\n",
    "\n",
    "config = tf.ConfigProto(inter_op_parallelism_threads=parallelization_factor,\n",
    "                      intra_op_parallelism_threads=parallelization_factor)\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "config.gpu_options.allow_growth=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tcga_no_brca = pd.read_pickle(\"../data/tcga_raw_no_labelled_brca_log_row_normalized.pkl\")\n",
    "X_brca_train = pd.read_pickle(\"../data/tcga_brca_raw_19036_row_log_norm_train.pkl\")\n",
    "y_brca_train = X_brca_train[\"Ciriello_subtype\"]\n",
    "X_brca_train.drop(['tcga_id', 'Ciriello_subtype', 'sample_id', 'cancer_type'], axis=\"columns\", inplace=True)\n",
    "\n",
    "X_brca_test = pd.read_pickle(\"../data/tcga_brca_raw_19036_row_log_norm_test.pkl\")\n",
    "y_brca_test = X_brca_test[\"subtype\"]\n",
    "\n",
    "X_brca_test.drop(['tcga_id', 'subtype', 'sample_id', 'cancer_type'], axis=\"columns\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_df = pd.DataFrame(columns=[\"accuracy\"])\n",
    "validation_set_percent = 0.1\n",
    "\n",
    "# Prepare data to train Variational Autoencoder (merge dataframes and normalize)\n",
    "X_autoencoder = pd.concat([X_brca_train, X_tcga_no_brca], sort=True)\n",
    "scaler = MinMaxScaler()\n",
    "X_autoencoder_scaled = pd.DataFrame(scaler.fit_transform(X_autoencoder), columns=X_autoencoder.columns)\n",
    "\n",
    "# Scale data\n",
    "X_brca_train_scaled = pd.DataFrame(scaler.transform(X_brca_train), columns=X_brca_train.columns)\n",
    "X_brca_test_scaled = pd.DataFrame(scaler.transform(X_brca_test), columns=X_brca_test.columns)\n",
    "\n",
    "X_autoencoder_scaled = X_autoencoder_scaled.reindex(sorted(X_autoencoder_scaled.columns), axis=\"columns\")\n",
    "X_brca_train_scaled = X_brca_train_scaled.reindex(sorted(X_brca_train_scaled.columns), axis=\"columns\")\n",
    "X_brca_test_scaled = X_brca_test_scaled.reindex(sorted(X_brca_test_scaled.columns), axis=\"columns\")\n",
    "\n",
    "X_autoencoder_val = X_autoencoder_scaled.sample(frac=validation_set_percent)\n",
    "X_autoencoder_train = X_autoencoder_scaled.drop(X_autoencoder_val.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize variables and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dim = X_autoencoder_scaled.shape[1]\n",
    "input_shape = (original_dim,)\n",
    "hidden_dim = 300\n",
    "latent_dim = 100\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "dropout_input = 0.6\n",
    "dropout_hidden = 0.6\n",
    "\n",
    "freeze_weights=False\n",
    "classifier_use_z=False\n",
    "noise_input=False\n",
    "reconstruction_loss='binary_crossentropy'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseVAE():\n",
    "\t\"\"\"docstring for BaseVAE\"\"\"\n",
    "\tdef __init__(self):\n",
    "\t\tpass\n",
    "\n",
    "\tdef initialize_model(self):\n",
    "\t\t\"\"\"\n",
    "\t\tHelper function to initialize the models\n",
    "\t\t\"\"\"\n",
    "\t\tself._build_encoder_layers()\n",
    "\t\tself._build_decoder_layers()\n",
    "\t\tself._compile_vae()\n",
    "\t\tself._compile_encoder_decoder()\n",
    "\n",
    "\tdef sampling(self, args):\n",
    "\n",
    "\t\t\"\"\"Reparameterization trick by sampling from an isotropic unit Gaussian.\n",
    "\t\t# Arguments\n",
    "\t\targs (tensor): mean and log of variance of Q(z|X)\n",
    "\t\t# Returns\n",
    "\t\tz (tensor): sampled latent vector\n",
    "\t\t\"\"\"\n",
    "\n",
    "\t\tz_mean, z_log_var = args\n",
    "\t\tbatch = K.shape(z_mean)[0]\n",
    "\t\tdim = K.int_shape(z_mean)[1]\n",
    "\t\t# by default, random_normal has mean = 0 and std = 1.0\n",
    "\t\tepsilon = K.random_normal(shape=(batch, dim), mean=0., stddev=1.)\n",
    "\t\t\n",
    "\t\treturn z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "\tdef vae_loss(self, y_true, y_pred):\n",
    "\n",
    "\t\t# E[log P(X|z)]\n",
    "\t\tif self.rec_loss == \"binary_crossentropy\":\n",
    "\t\t\treconstruction_loss = self.original_dim * binary_crossentropy(y_true, y_pred) # because it returns the mean cross-entropy\n",
    "\t\telif self.rec_loss == \"mse\":\n",
    "\t\t\treconstruction_loss = self.original_dim * mse(y_true, y_pred) # because it returns the mean cross-entropy\n",
    "\n",
    "\t\t# D_KL(Q(z|X) || P(z|X)); calculate in closed form as both dist. are Gaussian\n",
    "\t\tkl_loss = -0.5 * K.sum(1. + self.z_log_var_encoded - K.exp(self.z_log_var_encoded) - K.square(self.z_mean_encoded), axis=1)\n",
    "\n",
    "\t\treturn K.mean(reconstruction_loss + kl_loss)\n",
    "\n",
    "\n",
    "\tdef save_model(self, stacked=False):\n",
    "\t\tif(not stacked):\n",
    "\t\t\tmodel_name = \"../models/rna_vae_{}_hidden_{}_emb_{}_drop_in_{}_drop_hidden_noise_input_{}.h5\".format(self.intermediate_dim, self.latent_dim, self.dropout_rate_input, self.dropout_rate_hidden, self.noise_input)\n",
    "\t\telse:\n",
    "\t\t\tmodel_name = \"../models/rna_vae_{}_hidden_{}_emb_{}_drop_in_{}_drop_hidden_noise_input_{}_stacked_classifier.h5\".format(self.intermediate_dim, self.latent_dim, self.dropout_rate_input, self.dropout_rate_hidden, self.noise_input)\n",
    "\t\tself.vae.save_weights(model_name)\n",
    "\n",
    "\tdef load_model(self, filename):\n",
    "\t\tself.vae.load_weights(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(BaseVAE):\n",
    "\n",
    "\t\"\"\"\n",
    "    Building and Training a VAE)\n",
    "    Modified from:\n",
    "    https://github.com/greenelab/tybalt\n",
    "    \"\"\"\n",
    "\n",
    "\tdef __init__(self, original_dim,\n",
    "\t\t\t\t\t\tintermediate_dim=0,\n",
    "\t\t\t\t\t\tlatent_dim=100,\n",
    "\t\t\t\t\t\tepochs=100,\n",
    "\t\t\t\t\t\tbatch_size=50,\n",
    "\t\t\t\t\t\tlearning_rate=0.01,\n",
    "\t\t\t\t\t\tdropout_rate_input=0,\n",
    "\t\t\t\t\t\tdropout_rate_hidden=0,\n",
    "\t\t\t\t\t\tdropout_decoder=True,\n",
    "\t\t\t\t\t\tfreeze_weights=False,\n",
    "\t\t\t\t\t\tclassifier_use_z=False,\n",
    "\t\t\t\t\t\tnoise_input=True,\n",
    "\t\t\t\t\t\trec_loss=\"mse\",\n",
    "\t\t\t\t\t\tverbose=True):\n",
    "\n",
    "\t\tBaseVAE.__init__(self)\n",
    "\n",
    "\t\tself.original_dim = original_dim\n",
    "\t\tself.intermediate_dim = intermediate_dim\n",
    "\t\tself.latent_dim = latent_dim\n",
    "\t\tself.epochs = epochs\n",
    "\t\tself.batch_size = batch_size\n",
    "\t\tself.learning_rate = learning_rate\n",
    "\t\tself.dropout_rate_input = dropout_rate_input\n",
    "\t\tself.dropout_rate_hidden = dropout_rate_hidden\n",
    "\t\tself.dropout_decoder = dropout_decoder\n",
    "\t\tself.freeze_weights = freeze_weights\n",
    "\t\tself.classifier_use_z = classifier_use_z\n",
    "\t\tself.rec_loss = rec_loss\n",
    "\t\tself.verbose = verbose\n",
    "\t\tself.noise_input = noise_input\n",
    "\n",
    "\t\tself.depth = 2 if (intermediate_dim>0) else 1\n",
    "\n",
    "\t\tprint(\"AUTOENCODER HAS DEPTH {}\".format(self.depth))\n",
    "\n",
    "\tdef _build_encoder_layers(self):\n",
    "\t\tself.inputs = Input(shape=(self.original_dim, ), name=\"encoder_input\")\n",
    "\n",
    "\t\tif self.noise_input:\n",
    "\t\t\tnoise_layer_input = GaussianNoise(1)(self.inputs)\n",
    "\t\t\tdropout_input = Dropout(rate=self.dropout_rate_input)(noise_layer_input)\n",
    "\n",
    "\t\telse:\n",
    "\t\t\tdropout_input = Dropout(rate=self.dropout_rate_input)(self.inputs)\n",
    "\n",
    "\t\tif self.depth==1:\n",
    "\t\t\tz_mean_dense = Dense(self.latent_dim)(dropout_input)\n",
    "\t\t\tz_log_var_dense = Dense(self.latent_dim)(dropout_input)\n",
    "\n",
    "\t\telif self.depth==2:\n",
    "\t\t\thidden_dense = Dense(self.intermediate_dim)(dropout_input)\n",
    "\t\t\thidden_dense_batchnorm = BatchNormalization()(hidden_dense)\n",
    "\t\t\thidden_dense_encoded = Activation(\"relu\")(hidden_dense_batchnorm)\n",
    "\n",
    "\t\t\tdropout_encoder_hidden = Dropout(rate=self.dropout_rate_hidden)(hidden_dense_encoded)\n",
    "\n",
    "\t\t\tz_mean_dense = Dense(self.latent_dim)(dropout_encoder_hidden)\n",
    "\t\t\tz_log_var_dense = Dense(self.latent_dim)(dropout_encoder_hidden)\n",
    "\n",
    "\t\t# Latent representation layers\n",
    "\t\tz_mean_dense_batchnorm = BatchNormalization()(z_mean_dense)\n",
    "\t\tself.z_mean_encoded = Activation(\"relu\")(z_mean_dense_batchnorm)\n",
    "\n",
    "\t\tz_log_var_dense_batchnorm = BatchNormalization()(z_log_var_dense)\n",
    "\t\tself.z_log_var_encoded = Activation(\"relu\")(z_log_var_dense_batchnorm)\n",
    "\n",
    "\t\t# Sample z\n",
    "\t\tself.z = Lambda(self.sampling, output_shape=(self.latent_dim,), name=\"z\")([self.z_mean_encoded, self.z_log_var_encoded])\n",
    "\n",
    "\tdef _build_decoder_layers(self):\n",
    "\n",
    "\t\tif self.depth==1:\n",
    "\t\t\tself.decoder_output = Dense(self.original_dim, activation=\"sigmoid\", name=\"decoder_output\")\n",
    "\t\t\tself.outputs = self.decoder_output(self.z)\n",
    "\n",
    "\t\telif self.depth==2:\n",
    "\t\t\tself.decoder_hidden = Dense(self.intermediate_dim, activation=\"relu\", name=\"decoder_hidden\")\n",
    "\t\t\tself.decoder_output = Dense(self.original_dim, activation=\"sigmoid\", name=\"decoder_output\")\n",
    "\n",
    "\t\t\tvae_decoder_hidden = self.decoder_hidden(self.z)\n",
    "\t\t\tif self.dropout_decoder==True:\n",
    "\t\t\t\tself.decoder_dropout = Dropout(rate=self.dropout_rate_hidden)\n",
    "\n",
    "\t\t\t\tdecoder_hidden_dropout = self.decoder_dropout(vae_decoder_hidden)\n",
    "\t\t\t\tself.outputs = self.decoder_output(decoder_hidden_dropout)\n",
    "\n",
    "\t\t\telse:\n",
    "\t\t\t\tself.outputs = self.decoder_output(vae_decoder_hidden)\n",
    "\n",
    "\tdef _compile_encoder_decoder(self):\n",
    "\n",
    "\t\t# Compile Encoder\n",
    "\t\tself.encoder = Model(self.inputs, [self.z_mean_encoded, self.z_log_var_encoded], name=\"encoder\")\n",
    "\t\t'''\n",
    "\t\t# Compile Decoder\n",
    "\t\tdecoder_input = Input(shape=(self.latent_dim,), name='z_sampling')\n",
    "\n",
    "\t\tif self.depth==1:\n",
    "\t\t\tx_decoded = self.decoder_output(decoder_input)\n",
    "\t\telif self.depth==2:\n",
    "\t\t\tx_hidden = self.decoder_hidden(decoder_input)\n",
    "\t\t\tif self.dropout_decoder==True:\n",
    "\t\t\t\tx_dropout = self.decoder_dropout(x_hidden)\n",
    "\t\t\t\tx_decoded = self.decoder_output(x_dropout)\n",
    "\t\t\telse:\n",
    "\t\t\t\tX_decoded = self.decoder_output(x_hidden)\n",
    "\n",
    "\t\tself.decoder = Model(decoder_input, x_decoded, name='decoder')\n",
    "\t\t'''\n",
    "\n",
    "\tdef _compile_vae(self):\n",
    "\t\t\"\"\"\n",
    "\t\tCompiles all the layers together, creating the Variational Autoencoder\n",
    "\t\t\"\"\"\n",
    "\n",
    "\t\tadam = optimizers.Adam(lr=self.learning_rate)\n",
    "\t\tself.vae = Model(self.inputs, self.outputs, name='vae')\n",
    "\t\tself.vae.compile(optimizer=adam, loss=self.vae_loss)\n",
    "\n",
    "\tdef build_classifier(self):\n",
    "\n",
    "\t\tif(self.freeze_weights):\n",
    "\t\t\tfor layer in self.vae.layers:\n",
    "\t\t\t\tlayer.trainable = False\n",
    "\n",
    "\t\tif(self.classifier_use_z):\n",
    "\t\t\tself.classifier_output = Dense(5, activation=\"softmax\", name=\"classifier_output\")(self.z)\n",
    "\t\telse:\n",
    "\t\t\tself.classifier_output = Dense(5, activation=\"softmax\", name=\"classifier_output\")(concatenate([self.z_mean_encoded, self.z_log_var_encoded], axis=1))\n",
    "\n",
    "\t\tself.classifier = Model(self.inputs, self.classifier_output, name=\"classifier\")\n",
    "\t\t\n",
    "\t\tadam = optimizers.Adam(lr=self.learning_rate)\n",
    "\t\tself.classifier.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\tdef train_vae(self, train_df, val_df, val_flag=True):\n",
    "\t\tif(val_flag):\n",
    "\t\t\tself.train_hist = self.vae.fit(train_df, train_df,\n",
    "\t\t\t\t\t\t\tshuffle=True,\n",
    "\t\t\t\t\t\t\tepochs=self.epochs,\n",
    "\t\t\t\t\t\t\tbatch_size=self.batch_size,\n",
    "\t\t\t\t\t\t\tvalidation_data=(val_df, val_df),\n",
    "\t\t\t\t\t\t\tverbose=self.verbose)\n",
    "\t\telse:\n",
    "\t\t\tself.train_hist = self.vae.fit(train_df, train_df,\n",
    "\t\t\t\t\t\t\tshuffle=True,\n",
    "\t\t\t\t\t\t\tepochs=self.epochs,\n",
    "\t\t\t\t\t\t\tbatch_size=self.batch_size,\n",
    "\t\t\t\t\t\t\tverbose=self.verbose)\n",
    "\n",
    "\t\tself.hist_dataframe = pd.DataFrame(self.train_hist.history)\n",
    "\n",
    "\n",
    "\tdef train_stacked_classifier(self, train_df, val_df, epochs):\n",
    "\t\tself.classifier.fit(train_df=X_train, y_df=y_labels_train, epochs=epochs, verbose=self.verbose)\n",
    "\n",
    "\tdef evaluate_stacked_classifier(self, X_test, y_test):\n",
    "\t\tscore = self.classifier.evaluate(X_test, y_test)\n",
    "\t\treturn score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUTOENCODER HAS DEPTH 2\n"
     ]
    }
   ],
   "source": [
    "vae = VAE(original_dim=X_autoencoder_train.shape[1], \n",
    "        intermediate_dim=hidden_dim, \n",
    "        latent_dim=latent_dim, \n",
    "        epochs=epochs, \n",
    "        batch_size=batch_size, \n",
    "        learning_rate=learning_rate, \n",
    "        dropout_rate_input=dropout_input,\n",
    "        dropout_rate_hidden=dropout_hidden,\n",
    "        freeze_weights=freeze_weights, \n",
    "        classifier_use_z=classifier_use_z,\n",
    "        noise_input=noise_input,\n",
    "        rec_loss=reconstruction_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/cristovao/anaconda3/envs/thesis/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/cristovao/anaconda3/envs/thesis/lib/python3.7/site-packages/tensorflow/python/ops/control_flow_ops.py:423: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Train on 9094 samples, validate on 1010 samples\n",
      "WARNING:tensorflow:From /home/cristovao/anaconda3/envs/thesis/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/100\n",
      "9094/9094 [==============================] - 24s 3ms/sample - loss: 10451.6304 - val_loss: 10167.3697\n",
      "Epoch 2/100\n",
      "9094/9094 [==============================] - 21s 2ms/sample - loss: 10133.2945 - val_loss: 10034.6082\n",
      "Epoch 3/100\n",
      "9094/9094 [==============================] - 21s 2ms/sample - loss: 10073.2670 - val_loss: 9986.7663\n",
      "Epoch 4/100\n",
      "9094/9094 [==============================] - 20s 2ms/sample - loss: 10045.9097 - val_loss: 9948.2542\n",
      "Epoch 5/100\n",
      "9094/9094 [==============================] - 20s 2ms/sample - loss: 10023.1053 - val_loss: 9938.5029\n",
      "Epoch 6/100\n",
      "9094/9094 [==============================] - 20s 2ms/sample - loss: 10011.0791 - val_loss: 9920.6885\n",
      "Epoch 7/100\n",
      "9094/9094 [==============================] - 20s 2ms/sample - loss: 10003.4683 - val_loss: 9913.6596\n",
      "Epoch 8/100\n",
      "9094/9094 [==============================] - 20s 2ms/sample - loss: 9991.3061 - val_loss: 9898.4063\n",
      "Epoch 9/100\n",
      "9094/9094 [==============================] - 20s 2ms/sample - loss: 9985.1687 - val_loss: 9892.7822\n",
      "Epoch 10/100\n",
      "9094/9094 [==============================] - 20s 2ms/sample - loss: 9980.1414 - val_loss: 9886.8068\n",
      "Epoch 11/100\n",
      "9094/9094 [==============================] - 21s 2ms/sample - loss: 9973.8343 - val_loss: 9884.4705\n",
      "Epoch 12/100\n",
      "9094/9094 [==============================] - 21s 2ms/sample - loss: 9969.4665 - val_loss: 9880.3308\n",
      "Epoch 13/100\n",
      "9094/9094 [==============================] - 19s 2ms/sample - loss: 9964.9755 - val_loss: 9877.8377\n",
      "Epoch 14/100\n",
      "9094/9094 [==============================] - 20s 2ms/sample - loss: 9963.9933 - val_loss: 9873.8984\n",
      "Epoch 15/100\n",
      "9094/9094 [==============================] - 19s 2ms/sample - loss: 9959.0251 - val_loss: 9870.6052\n",
      "Epoch 16/100\n",
      "9094/9094 [==============================] - 20s 2ms/sample - loss: 9957.2114 - val_loss: 9866.5300\n",
      "Epoch 17/100\n",
      "9094/9094 [==============================] - 21s 2ms/sample - loss: 9955.1990 - val_loss: 9865.3573\n",
      "Epoch 18/100\n",
      "9094/9094 [==============================] - 21s 2ms/sample - loss: 9952.7782 - val_loss: 9864.2978\n",
      "Epoch 19/100\n",
      "9094/9094 [==============================] - 21s 2ms/sample - loss: 9948.1790 - val_loss: 9861.9540\n",
      "Epoch 20/100\n",
      "9094/9094 [==============================] - 21s 2ms/sample - loss: 9948.6471 - val_loss: 9858.6968\n",
      "Epoch 21/100\n",
      "9094/9094 [==============================] - 21s 2ms/sample - loss: 9944.5069 - val_loss: 9857.9051\n",
      "Epoch 22/100\n",
      "9094/9094 [==============================] - 20s 2ms/sample - loss: 9943.3050 - val_loss: 9854.8798\n",
      "Epoch 23/100\n",
      "9094/9094 [==============================] - 20s 2ms/sample - loss: 9939.6931 - val_loss: 9855.6412\n",
      "Epoch 24/100\n",
      "9094/9094 [==============================] - 21s 2ms/sample - loss: 9938.1828 - val_loss: 9853.3700\n",
      "Epoch 25/100\n",
      "9094/9094 [==============================] - 20s 2ms/sample - loss: 9939.4548 - val_loss: 9851.2432\n",
      "Epoch 26/100\n",
      "9094/9094 [==============================] - 20s 2ms/sample - loss: 9934.5208 - val_loss: 9851.8943\n",
      "Epoch 27/100\n",
      "9094/9094 [==============================] - 20s 2ms/sample - loss: 9933.0806 - val_loss: 9849.7484\n",
      "Epoch 28/100\n",
      "9094/9094 [==============================] - 20s 2ms/sample - loss: 9932.5850 - val_loss: 9846.8013\n",
      "Epoch 29/100\n",
      "9094/9094 [==============================] - 20s 2ms/sample - loss: 9930.9339 - val_loss: 9847.6756\n",
      "Epoch 30/100\n",
      "9094/9094 [==============================] - 20s 2ms/sample - loss: 9929.7916 - val_loss: 9844.4358\n",
      "Epoch 31/100\n",
      "9094/9094 [==============================] - 20s 2ms/sample - loss: 9928.8145 - val_loss: 9846.2017\n",
      "Epoch 32/100\n",
      "9094/9094 [==============================] - 19s 2ms/sample - loss: 9929.7160 - val_loss: 9843.0216\n",
      "Epoch 33/100\n",
      "9094/9094 [==============================] - 19s 2ms/sample - loss: 9926.5385 - val_loss: 9843.9224\n",
      "Epoch 34/100\n",
      "9094/9094 [==============================] - 20s 2ms/sample - loss: 9924.5445 - val_loss: 9841.9331\n",
      "Epoch 35/100\n",
      "9094/9094 [==============================] - 20s 2ms/sample - loss: 9924.0643 - val_loss: 9843.2519\n",
      "Epoch 36/100\n",
      "9094/9094 [==============================] - 21s 2ms/sample - loss: 9924.6366 - val_loss: 9844.1945\n",
      "Epoch 37/100\n",
      "9094/9094 [==============================] - 20s 2ms/sample - loss: 9923.6485 - val_loss: 9840.0485\n",
      "Epoch 38/100\n",
      "9094/9094 [==============================] - 20s 2ms/sample - loss: 9922.6308 - val_loss: 9840.8675\n",
      "Epoch 39/100\n",
      "9094/9094 [==============================] - 19s 2ms/sample - loss: 9921.4240 - val_loss: 9838.4311\n",
      "Epoch 40/100\n",
      "9094/9094 [==============================] - 21s 2ms/sample - loss: 9922.4761 - val_loss: 9839.5719\n",
      "Epoch 41/100\n",
      "9094/9094 [==============================] - 20s 2ms/sample - loss: 9918.8589 - val_loss: 9840.7096\n",
      "Epoch 42/100\n",
      "9094/9094 [==============================] - 20s 2ms/sample - loss: 9918.9277 - val_loss: 9836.2804\n",
      "Epoch 43/100\n",
      "9094/9094 [==============================] - 20s 2ms/sample - loss: 9919.9829 - val_loss: 9837.0737\n",
      "Epoch 44/100\n",
      "9094/9094 [==============================] - 21s 2ms/sample - loss: 9917.5346 - val_loss: 9835.5919\n",
      "Epoch 45/100\n",
      "9094/9094 [==============================] - 21s 2ms/sample - loss: 9916.9033 - val_loss: 9834.0574\n",
      "Epoch 46/100\n",
      "9094/9094 [==============================] - 21s 2ms/sample - loss: 9916.3994 - val_loss: 9835.6327\n",
      "Epoch 47/100\n",
      "9094/9094 [==============================] - 20s 2ms/sample - loss: 9915.4069 - val_loss: 9833.8079\n",
      "Epoch 48/100\n",
      "9094/9094 [==============================] - 20s 2ms/sample - loss: 9914.9041 - val_loss: 9835.0070\n",
      "Epoch 49/100\n",
      "9094/9094 [==============================] - 20s 2ms/sample - loss: 9914.3770 - val_loss: 9832.6987\n",
      "Epoch 50/100\n",
      "9094/9094 [==============================] - 20s 2ms/sample - loss: 9913.1323 - val_loss: 9833.6651\n",
      "Epoch 51/100\n",
      "9094/9094 [==============================] - 20s 2ms/sample - loss: 9913.5959 - val_loss: 9832.6682\n",
      "Epoch 52/100\n",
      "9094/9094 [==============================] - 20s 2ms/sample - loss: 9912.9320 - val_loss: 9832.6397\n",
      "Epoch 53/100\n",
      "9094/9094 [==============================] - 21s 2ms/sample - loss: 9915.0143 - val_loss: 9835.5996\n",
      "Epoch 54/100\n",
      "9094/9094 [==============================] - 20s 2ms/sample - loss: 9912.7944 - val_loss: 9831.4776\n",
      "Epoch 55/100\n",
      "9094/9094 [==============================] - 20s 2ms/sample - loss: 9912.1041 - val_loss: 9833.2003\n",
      "Epoch 56/100\n",
      "9094/9094 [==============================] - 19s 2ms/sample - loss: 9911.0124 - val_loss: 9831.1667\n",
      "Epoch 57/100\n",
      "9094/9094 [==============================] - 19s 2ms/sample - loss: 9911.2460 - val_loss: 9831.8758\n",
      "Epoch 58/100\n",
      "9094/9094 [==============================] - 19s 2ms/sample - loss: 9909.0930 - val_loss: 9831.2301\n",
      "Epoch 59/100\n",
      "9094/9094 [==============================] - 19s 2ms/sample - loss: 9908.9117 - val_loss: 9831.4369\n",
      "Epoch 60/100\n",
      "9094/9094 [==============================] - 19s 2ms/sample - loss: 9908.5609 - val_loss: 9831.1442\n",
      "Epoch 61/100\n",
      "9094/9094 [==============================] - 19s 2ms/sample - loss: 9906.5256 - val_loss: 9830.2178\n",
      "Epoch 62/100\n",
      "9094/9094 [==============================] - 19s 2ms/sample - loss: 9907.6824 - val_loss: 9829.1674\n",
      "Epoch 63/100\n",
      "9094/9094 [==============================] - 19s 2ms/sample - loss: 9906.6989 - val_loss: 9829.0297\n",
      "Epoch 64/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9094/9094 [==============================] - 20s 2ms/sample - loss: 9907.2273 - val_loss: 9827.4751\n",
      "Epoch 65/100\n",
      "9094/9094 [==============================] - 20s 2ms/sample - loss: 9905.9882 - val_loss: 9829.5702\n",
      "Epoch 66/100\n",
      "9094/9094 [==============================] - 20s 2ms/sample - loss: 9906.5182 - val_loss: 9828.6991\n",
      "Epoch 67/100\n",
      "9094/9094 [==============================] - 20s 2ms/sample - loss: 9906.7131 - val_loss: 9828.6307\n",
      "Epoch 68/100\n",
      "9094/9094 [==============================] - 20s 2ms/sample - loss: 9906.7805 - val_loss: 9826.9570\n",
      "Epoch 69/100\n",
      "9094/9094 [==============================] - 20s 2ms/sample - loss: 9906.5885 - val_loss: 9827.6608\n",
      "Epoch 70/100\n",
      "9094/9094 [==============================] - 20s 2ms/sample - loss: 9905.7950 - val_loss: 9827.1824\n",
      "Epoch 71/100\n",
      "9094/9094 [==============================] - 20s 2ms/sample - loss: 9906.6149 - val_loss: 9830.7397\n",
      "Epoch 72/100\n",
      "9094/9094 [==============================] - 20s 2ms/sample - loss: 9905.0603 - val_loss: 9825.5998\n",
      "Epoch 73/100\n",
      "9094/9094 [==============================] - 20s 2ms/sample - loss: 9904.2353 - val_loss: 9823.8602\n",
      "Epoch 74/100\n",
      "9094/9094 [==============================] - 20s 2ms/sample - loss: 9902.5898 - val_loss: 9826.0539\n",
      "Epoch 75/100\n",
      "9094/9094 [==============================] - 19s 2ms/sample - loss: 9904.5905 - val_loss: 9824.5652\n",
      "Epoch 76/100\n",
      "9094/9094 [==============================] - 20s 2ms/sample - loss: 9903.0686 - val_loss: 9827.0219\n",
      "Epoch 77/100\n",
      "9094/9094 [==============================] - 20s 2ms/sample - loss: 9902.0558 - val_loss: 9826.8108\n",
      "Epoch 78/100\n",
      "9094/9094 [==============================] - 20s 2ms/sample - loss: 9905.3961 - val_loss: 9827.0480\n",
      "Epoch 79/100\n",
      "9094/9094 [==============================] - 20s 2ms/sample - loss: 9903.1216 - val_loss: 9826.9066\n",
      "Epoch 80/100\n",
      "9094/9094 [==============================] - 20s 2ms/sample - loss: 9902.1466 - val_loss: 9825.8866\n",
      "Epoch 81/100\n",
      "9094/9094 [==============================] - 20s 2ms/sample - loss: 9901.1070 - val_loss: 9825.8145\n",
      "Epoch 82/100\n",
      "9094/9094 [==============================] - 20s 2ms/sample - loss: 9902.7333 - val_loss: 9825.5921\n",
      "Epoch 83/100\n",
      "9094/9094 [==============================] - 20s 2ms/sample - loss: 9901.1706 - val_loss: 9822.9301\n",
      "Epoch 84/100\n",
      "9094/9094 [==============================] - 20s 2ms/sample - loss: 9901.3605 - val_loss: 9824.3010\n",
      "Epoch 85/100\n",
      "9094/9094 [==============================] - 19s 2ms/sample - loss: 9899.8044 - val_loss: 9822.4044\n",
      "Epoch 86/100\n",
      "9094/9094 [==============================] - 19s 2ms/sample - loss: 9901.9424 - val_loss: 9824.5846\n",
      "Epoch 87/100\n",
      "9094/9094 [==============================] - 20s 2ms/sample - loss: 9900.6830 - val_loss: 9825.1361\n",
      "Epoch 88/100\n",
      "9094/9094 [==============================] - 20s 2ms/sample - loss: 9902.4108 - val_loss: 9823.3537\n",
      "Epoch 89/100\n",
      "9094/9094 [==============================] - 20s 2ms/sample - loss: 9901.1046 - val_loss: 9823.1222\n",
      "Epoch 90/100\n",
      "9094/9094 [==============================] - 20s 2ms/sample - loss: 9899.4672 - val_loss: 9822.4492\n",
      "Epoch 91/100\n",
      "9094/9094 [==============================] - 19s 2ms/sample - loss: 9901.4490 - val_loss: 9824.5032\n",
      "Epoch 92/100\n",
      "9094/9094 [==============================] - 19s 2ms/sample - loss: 9901.1544 - val_loss: 9822.2555\n",
      "Epoch 93/100\n",
      "9094/9094 [==============================] - 19s 2ms/sample - loss: 9899.5287 - val_loss: 9821.5548\n",
      "Epoch 94/100\n",
      "9094/9094 [==============================] - 20s 2ms/sample - loss: 9897.8523 - val_loss: 9820.4287\n",
      "Epoch 95/100\n",
      "9094/9094 [==============================] - 20s 2ms/sample - loss: 9899.2047 - val_loss: 9822.3392\n",
      "Epoch 96/100\n",
      "9094/9094 [==============================] - 20s 2ms/sample - loss: 9900.5007 - val_loss: 9824.7050\n",
      "Epoch 97/100\n",
      "9094/9094 [==============================] - 19s 2ms/sample - loss: 9899.5579 - val_loss: 9823.3050\n",
      "Epoch 98/100\n",
      "9094/9094 [==============================] - 20s 2ms/sample - loss: 9898.0717 - val_loss: 9821.1425\n",
      "Epoch 99/100\n",
      "9094/9094 [==============================] - 20s 2ms/sample - loss: 9898.6173 - val_loss: 9824.3080\n",
      "Epoch 100/100\n",
      "9094/9094 [==============================] - 20s 2ms/sample - loss: 9898.6549 - val_loss: 9820.8889\n"
     ]
    }
   ],
   "source": [
    "vae.initialize_model()\n",
    "vae.train_vae(train_df=X_autoencoder_train, val_df=X_autoencoder_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BUILDING CLASSIFIER\n",
      "Train on 735 samples, validate on 82 samples\n",
      "Epoch 1/100\n",
      "735/735 [==============================] - 1s 2ms/sample - loss: 1.6418 - acc: 0.2612 - val_loss: 1.4102 - val_acc: 0.2317\n",
      "Epoch 2/100\n",
      "735/735 [==============================] - 1s 1ms/sample - loss: 1.2205 - acc: 0.5687 - val_loss: 1.0630 - val_acc: 0.6707\n",
      "Epoch 3/100\n",
      "735/735 [==============================] - 1s 1ms/sample - loss: 0.9828 - acc: 0.7034 - val_loss: 0.9105 - val_acc: 0.6585\n",
      "Epoch 4/100\n",
      "735/735 [==============================] - 1s 899us/sample - loss: 0.8545 - acc: 0.7184 - val_loss: 0.8772 - val_acc: 0.6585\n",
      "Epoch 5/100\n",
      "735/735 [==============================] - 1s 888us/sample - loss: 0.7495 - acc: 0.7469 - val_loss: 0.8726 - val_acc: 0.6707\n",
      "Epoch 6/100\n",
      "735/735 [==============================] - 1s 996us/sample - loss: 0.6892 - acc: 0.7633 - val_loss: 0.8415 - val_acc: 0.7195\n",
      "Epoch 7/100\n",
      "735/735 [==============================] - 1s 868us/sample - loss: 0.6352 - acc: 0.7660 - val_loss: 0.8465 - val_acc: 0.7073\n",
      "Epoch 8/100\n",
      "735/735 [==============================] - 1s 939us/sample - loss: 0.6325 - acc: 0.7565 - val_loss: 0.8429 - val_acc: 0.7073\n",
      "Epoch 9/100\n",
      "735/735 [==============================] - 1s 933us/sample - loss: 0.6101 - acc: 0.7755 - val_loss: 0.7957 - val_acc: 0.7073\n",
      "Epoch 10/100\n",
      "735/735 [==============================] - 1s 997us/sample - loss: 0.5563 - acc: 0.8000 - val_loss: 0.7378 - val_acc: 0.7073\n",
      "Epoch 11/100\n",
      "735/735 [==============================] - 1s 949us/sample - loss: 0.5700 - acc: 0.7878 - val_loss: 0.6901 - val_acc: 0.7439\n",
      "Epoch 12/100\n",
      "735/735 [==============================] - 1s 885us/sample - loss: 0.5211 - acc: 0.8136 - val_loss: 0.6661 - val_acc: 0.7561\n",
      "Epoch 13/100\n",
      "735/735 [==============================] - 1s 1ms/sample - loss: 0.5150 - acc: 0.8000 - val_loss: 0.6467 - val_acc: 0.7927\n",
      "Epoch 14/100\n",
      "735/735 [==============================] - 1s 995us/sample - loss: 0.4815 - acc: 0.8245 - val_loss: 0.6257 - val_acc: 0.7927\n",
      "Epoch 15/100\n",
      "735/735 [==============================] - 1s 987us/sample - loss: 0.4958 - acc: 0.8150 - val_loss: 0.5985 - val_acc: 0.7927\n",
      "Epoch 16/100\n",
      "735/735 [==============================] - 1s 957us/sample - loss: 0.4585 - acc: 0.8245 - val_loss: 0.5348 - val_acc: 0.8049\n",
      "Epoch 17/100\n",
      "735/735 [==============================] - 1s 852us/sample - loss: 0.4353 - acc: 0.8354 - val_loss: 0.5104 - val_acc: 0.8049\n",
      "Epoch 18/100\n",
      "735/735 [==============================] - 1s 915us/sample - loss: 0.4181 - acc: 0.8340 - val_loss: 0.4835 - val_acc: 0.8049\n",
      "Epoch 19/100\n",
      "735/735 [==============================] - 1s 989us/sample - loss: 0.4205 - acc: 0.8327 - val_loss: 0.4636 - val_acc: 0.8171\n",
      "Epoch 20/100\n",
      "735/735 [==============================] - 1s 915us/sample - loss: 0.4569 - acc: 0.8245 - val_loss: 0.4530 - val_acc: 0.8049\n",
      "Epoch 21/100\n",
      "735/735 [==============================] - 1s 932us/sample - loss: 0.3840 - acc: 0.8571 - val_loss: 0.3918 - val_acc: 0.8293\n",
      "Epoch 22/100\n",
      "735/735 [==============================] - 1s 857us/sample - loss: 0.4007 - acc: 0.8517 - val_loss: 0.3854 - val_acc: 0.8293\n",
      "Epoch 23/100\n",
      "735/735 [==============================] - 1s 930us/sample - loss: 0.3819 - acc: 0.8476 - val_loss: 0.3454 - val_acc: 0.8537\n",
      "Epoch 24/100\n",
      "735/735 [==============================] - 1s 856us/sample - loss: 0.3805 - acc: 0.8517 - val_loss: 0.3317 - val_acc: 0.8537\n",
      "Epoch 25/100\n",
      "735/735 [==============================] - 1s 925us/sample - loss: 0.3600 - acc: 0.8585 - val_loss: 0.3079 - val_acc: 0.8780\n",
      "Epoch 26/100\n",
      "735/735 [==============================] - 1s 1ms/sample - loss: 0.3750 - acc: 0.8531 - val_loss: 0.3170 - val_acc: 0.8537\n",
      "Epoch 27/100\n",
      "735/735 [==============================] - 1s 1ms/sample - loss: 0.3696 - acc: 0.8639 - val_loss: 0.2992 - val_acc: 0.8537\n",
      "Epoch 28/100\n",
      "735/735 [==============================] - 1s 1ms/sample - loss: 0.3502 - acc: 0.8571 - val_loss: 0.2791 - val_acc: 0.8780\n",
      "Epoch 29/100\n",
      "735/735 [==============================] - 1s 957us/sample - loss: 0.3066 - acc: 0.8830 - val_loss: 0.2673 - val_acc: 0.8902\n",
      "Epoch 30/100\n",
      "735/735 [==============================] - 1s 856us/sample - loss: 0.3192 - acc: 0.8776 - val_loss: 0.2599 - val_acc: 0.8780\n",
      "Epoch 31/100\n",
      "735/735 [==============================] - 1s 842us/sample - loss: 0.3189 - acc: 0.8762 - val_loss: 0.2473 - val_acc: 0.8902\n",
      "Epoch 32/100\n",
      "735/735 [==============================] - 1s 1ms/sample - loss: 0.3250 - acc: 0.8871 - val_loss: 0.2305 - val_acc: 0.9146\n",
      "Epoch 33/100\n",
      "735/735 [==============================] - 1s 1ms/sample - loss: 0.3114 - acc: 0.8816 - val_loss: 0.2219 - val_acc: 0.9146\n",
      "Epoch 34/100\n",
      "735/735 [==============================] - 1s 973us/sample - loss: 0.2951 - acc: 0.8735 - val_loss: 0.2199 - val_acc: 0.9146\n",
      "Epoch 35/100\n",
      "735/735 [==============================] - 1s 940us/sample - loss: 0.2991 - acc: 0.8871 - val_loss: 0.2252 - val_acc: 0.9024\n",
      "Epoch 36/100\n",
      "735/735 [==============================] - 1s 950us/sample - loss: 0.3262 - acc: 0.8667 - val_loss: 0.2310 - val_acc: 0.9024\n",
      "Epoch 37/100\n",
      "735/735 [==============================] - 1s 923us/sample - loss: 0.3021 - acc: 0.8884 - val_loss: 0.2288 - val_acc: 0.9024\n",
      "Epoch 38/100\n",
      "735/735 [==============================] - 1s 939us/sample - loss: 0.3181 - acc: 0.8857 - val_loss: 0.2096 - val_acc: 0.9024\n",
      "Epoch 39/100\n",
      "735/735 [==============================] - 1s 932us/sample - loss: 0.2951 - acc: 0.8844 - val_loss: 0.2102 - val_acc: 0.9024\n",
      "Epoch 40/100\n",
      "735/735 [==============================] - 1s 860us/sample - loss: 0.2954 - acc: 0.8748 - val_loss: 0.2117 - val_acc: 0.9024\n",
      "Epoch 41/100\n",
      "735/735 [==============================] - 1s 869us/sample - loss: 0.2887 - acc: 0.8830 - val_loss: 0.2052 - val_acc: 0.9146\n",
      "Epoch 42/100\n",
      "735/735 [==============================] - 1s 826us/sample - loss: 0.2973 - acc: 0.8993 - val_loss: 0.2080 - val_acc: 0.9146\n",
      "Epoch 43/100\n",
      "735/735 [==============================] - 1s 828us/sample - loss: 0.2796 - acc: 0.8884 - val_loss: 0.2164 - val_acc: 0.9024\n",
      "Epoch 44/100\n",
      "735/735 [==============================] - 1s 852us/sample - loss: 0.2631 - acc: 0.8966 - val_loss: 0.2033 - val_acc: 0.9146\n",
      "Epoch 45/100\n",
      "735/735 [==============================] - 1s 852us/sample - loss: 0.2570 - acc: 0.9129 - val_loss: 0.2201 - val_acc: 0.9024\n",
      "Epoch 46/100\n",
      "735/735 [==============================] - 1s 914us/sample - loss: 0.2591 - acc: 0.9034 - val_loss: 0.2367 - val_acc: 0.8902\n",
      "Epoch 47/100\n",
      "735/735 [==============================] - 1s 871us/sample - loss: 0.2728 - acc: 0.8980 - val_loss: 0.2382 - val_acc: 0.8902\n",
      "Epoch 48/100\n",
      "735/735 [==============================] - 1s 857us/sample - loss: 0.2541 - acc: 0.9048 - val_loss: 0.2187 - val_acc: 0.9024\n",
      "Epoch 49/100\n",
      "735/735 [==============================] - 1s 951us/sample - loss: 0.2589 - acc: 0.8952 - val_loss: 0.2141 - val_acc: 0.9024\n",
      "Epoch 50/100\n",
      "735/735 [==============================] - 1s 1ms/sample - loss: 0.2665 - acc: 0.8952 - val_loss: 0.1997 - val_acc: 0.9268\n",
      "Epoch 51/100\n",
      "735/735 [==============================] - 1s 1ms/sample - loss: 0.2618 - acc: 0.9075 - val_loss: 0.1922 - val_acc: 0.9268\n",
      "Epoch 52/100\n",
      "735/735 [==============================] - 1s 966us/sample - loss: 0.2321 - acc: 0.9129 - val_loss: 0.2035 - val_acc: 0.9268\n",
      "Epoch 53/100\n",
      "735/735 [==============================] - 1s 957us/sample - loss: 0.2392 - acc: 0.9048 - val_loss: 0.2014 - val_acc: 0.9268\n",
      "Epoch 54/100\n",
      "735/735 [==============================] - 1s 924us/sample - loss: 0.2764 - acc: 0.8776 - val_loss: 0.2060 - val_acc: 0.9268\n",
      "Epoch 55/100\n",
      "735/735 [==============================] - 1s 1ms/sample - loss: 0.2386 - acc: 0.9007 - val_loss: 0.1781 - val_acc: 0.9268\n",
      "Epoch 56/100\n",
      "735/735 [==============================] - 1s 1ms/sample - loss: 0.2224 - acc: 0.9211 - val_loss: 0.1773 - val_acc: 0.9268\n",
      "Epoch 57/100\n",
      "735/735 [==============================] - 1s 828us/sample - loss: 0.2504 - acc: 0.9048 - val_loss: 0.1742 - val_acc: 0.9268\n",
      "Epoch 58/100\n",
      "735/735 [==============================] - 1s 924us/sample - loss: 0.2001 - acc: 0.9252 - val_loss: 0.1679 - val_acc: 0.9268\n",
      "Epoch 59/100\n",
      "735/735 [==============================] - 1s 1ms/sample - loss: 0.2304 - acc: 0.9020 - val_loss: 0.1715 - val_acc: 0.9268\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "735/735 [==============================] - 1s 1ms/sample - loss: 0.1917 - acc: 0.9224 - val_loss: 0.1831 - val_acc: 0.9268\n",
      "Epoch 61/100\n",
      "735/735 [==============================] - 1s 989us/sample - loss: 0.2260 - acc: 0.9143 - val_loss: 0.1738 - val_acc: 0.9390\n",
      "Epoch 62/100\n",
      "735/735 [==============================] - 1s 969us/sample - loss: 0.2401 - acc: 0.8912 - val_loss: 0.1809 - val_acc: 0.9146\n",
      "Epoch 63/100\n",
      "735/735 [==============================] - 1s 957us/sample - loss: 0.2144 - acc: 0.9197 - val_loss: 0.1691 - val_acc: 0.9390\n",
      "Epoch 64/100\n",
      "735/735 [==============================] - 1s 928us/sample - loss: 0.1951 - acc: 0.9184 - val_loss: 0.1670 - val_acc: 0.9390\n",
      "Epoch 65/100\n",
      "735/735 [==============================] - 1s 916us/sample - loss: 0.2311 - acc: 0.9061 - val_loss: 0.1897 - val_acc: 0.9390\n",
      "Epoch 66/100\n",
      "735/735 [==============================] - 1s 927us/sample - loss: 0.1855 - acc: 0.9293 - val_loss: 0.1978 - val_acc: 0.9268\n",
      "Epoch 67/100\n",
      "735/735 [==============================] - 1s 885us/sample - loss: 0.2327 - acc: 0.9061 - val_loss: 0.1867 - val_acc: 0.9390\n",
      "Epoch 68/100\n",
      "735/735 [==============================] - 1s 873us/sample - loss: 0.1964 - acc: 0.9279 - val_loss: 0.1832 - val_acc: 0.9390\n",
      "Epoch 69/100\n",
      "735/735 [==============================] - 1s 894us/sample - loss: 0.2122 - acc: 0.9238 - val_loss: 0.1915 - val_acc: 0.9268\n",
      "Epoch 70/100\n",
      "735/735 [==============================] - 1s 917us/sample - loss: 0.2260 - acc: 0.9129 - val_loss: 0.2141 - val_acc: 0.9146\n",
      "Epoch 71/100\n",
      "735/735 [==============================] - 1s 1ms/sample - loss: 0.2139 - acc: 0.8980 - val_loss: 0.1879 - val_acc: 0.9390\n",
      "Epoch 72/100\n",
      "735/735 [==============================] - 1s 988us/sample - loss: 0.1945 - acc: 0.9238 - val_loss: 0.1881 - val_acc: 0.9268\n",
      "Epoch 73/100\n",
      "735/735 [==============================] - 1s 896us/sample - loss: 0.1999 - acc: 0.9211 - val_loss: 0.1811 - val_acc: 0.9390\n",
      "Epoch 74/100\n",
      "735/735 [==============================] - 1s 1ms/sample - loss: 0.1967 - acc: 0.9279 - val_loss: 0.1700 - val_acc: 0.9390\n"
     ]
    }
   ],
   "source": [
    "enc = OneHotEncoder()\n",
    "y_labels_train = enc.fit_transform(y_brca_train.values.reshape(-1, 1))\n",
    "y_labels_test = enc.fit_transform(y_brca_test.values.reshape(-1, 1))\n",
    "\n",
    "X_train_train, X_train_val, y_labels_train_train, y_labels_train_val = train_test_split(X_brca_train_scaled, y_labels_train, test_size=0.1, stratify=y_brca_train, random_state=42)\n",
    "\n",
    "print(\"BUILDING CLASSIFIER\")\n",
    "vae.build_classifier()\n",
    "\n",
    "fit_hist = vae.classifier.fit(x=X_train_train, \n",
    "\t\t\t\t\t\t\t\ty=y_labels_train_train, \n",
    "\t\t\t\t\t\t\t\tshuffle=True, \n",
    "\t\t\t\t\t\t\t\tepochs=100,\n",
    "\t\t\t\t\t\t\t\tbatch_size=64,\n",
    "\t\t\t\t\t\t\t\tcallbacks=[EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)],\n",
    "\t\t\t\t\t\t\t\tvalidation_data=(X_train_val, y_labels_train_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      (None, 19036)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 19036)        0           encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 300)          5711100     dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1 (BatchNo (None, 300)          1200        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 300)          0           batch_normalization_v1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 300)          0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 100)          30100       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 100)          30100       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_1 (Batch (None, 100)          400         dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_2 (Batch (None, 100)          400         dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 100)          0           batch_normalization_v1_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 100)          0           batch_normalization_v1_2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 200)          0           activation_1[0][0]               \n",
      "                                                                 activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "classifier_output (Dense)       (None, 5)            1005        concatenate_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 5,774,305\n",
      "Trainable params: 5,773,305\n",
      "Non-trainable params: 1,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vae.classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    " = vae.classifier.layers[2].get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_dense_weights[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../vae_weights_analysis.npy', first_dense_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../vae_weights_analysis_genes.npy', X_tcga_no_brca.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "asdad = np.load('../vae_weights_analysis.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.06588272, -0.00957081,  0.15896752, ...,  0.03977008,\n",
       "         0.17044045,  0.16687728],\n",
       "       [-0.03123969,  0.14270912,  0.11946226, ..., -0.03190156,\n",
       "        -0.02870585, -0.02252444],\n",
       "       [-0.1168997 , -0.22999266,  0.03431503, ...,  0.06029829,\n",
       "        -0.13417697,  0.10309508],\n",
       "       ...,\n",
       "       [ 0.10308177, -0.10115288,  0.02363555, ..., -0.08729688,\n",
       "         0.02522375, -0.05210141],\n",
       "       [-0.08821324,  0.07808172,  0.06051018, ..., -0.06800039,\n",
       "         0.10620739, -0.08172274],\n",
       "       [-0.19392179,  0.16963923, -0.1673271 , ...,  0.09801459,\n",
       "         0.02606442, -0.14854248]], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asdad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_weights_grouped = [np.sum(np.absolute(arr)) for arr in first_dense_weights]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19036"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gene_weights_grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19036"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_tcga_no_brca.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../vae_weights_analysis_grouped_absolute.npy', gene_weights_grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
